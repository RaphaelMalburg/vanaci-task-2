import { NextRequest, NextResponse } from 'next/server';
import { getPharmacyAgent } from '@/lib/ai-agent';
import { generateId } from '@/lib/ai-agent/utils';

// Configura√ß√£o para permitir streaming
export const runtime = 'nodejs';
export const dynamic = 'force-dynamic';

// POST - Processar mensagem do usu√°rio
export async function POST(request: NextRequest) {
  try {
    console.log('üöÄ API ai-chat: Recebendo requisi√ß√£o POST');
    const body = await request.json();
    console.log('üì¶ Dados recebidos:', JSON.stringify(body, null, 2));
    
    const { message, sessionId, context, streaming = true, llmConfig } = body;

    // Valida√ß√µes
    if (!message || typeof message !== 'string') {
      console.log('‚ùå Erro: Mensagem inv√°lida ou ausente');
      return NextResponse.json(
        { error: 'Mensagem √© obrigat√≥ria' },
        { status: 400 }
      );
    }

    // Gerar sessionId se n√£o fornecido
    const finalSessionId = sessionId || generateId();
    console.log('üÜî Session ID:', finalSessionId);
    console.log('üìù Mensagem:', message);
    console.log('üîÑ Streaming habilitado:', streaming);

    // Configura√ß√£o do LLM - usando Google Gemini como padr√£o
    const finalLlmConfig = llmConfig || {
      provider: 'google' as const,
      temperature: 0.7,
      maxTokens: 2000,
    };
    console.log('‚öôÔ∏è Configura√ß√£o LLM:', finalLlmConfig);

    // Obter inst√¢ncia do agente
    console.log('ü§ñ Criando inst√¢ncia do agente...');
    const agent = getPharmacyAgent(finalLlmConfig);
    console.log('‚úÖ Agente criado com sucesso');

    // Processar com ou sem streaming
    if (streaming) {
      console.log('üåä Iniciando processamento com streaming...');
      // Streaming response
      const streamResult = await agent.streamMessage(finalSessionId, message, context);
      console.log('üì° Stream result obtido:', !!streamResult);
      
      // Converter para ReadableStream
      const stream = new ReadableStream({
        async start(controller) {
          try {
            console.log('üöÄ Iniciando stream para sess√£o:', finalSessionId);
            console.log('üì° Stream result obtido:', !!streamResult);
            console.log('üì° Stream result keys:', Object.keys(streamResult));

            // Log detalhado do resultado
            if (streamResult.toolCalls) {
              console.log('üîß Tool calls promise detectado no resultado inicial');
              try {
                const toolCallsResolved = await streamResult.toolCalls;
                console.log('üîß Tool calls no resultado inicial:', toolCallsResolved?.length || 0);
                if (toolCallsResolved && toolCallsResolved.length > 0) {
                   toolCallsResolved.forEach((tc, index) => {
                     console.log(`üõ†Ô∏è Tool Call ${index}:`, {
                       toolName: tc.toolName,
                       toolCallId: tc.toolCallId
                     });
                   });
                 }
              } catch (toolError) {
                console.error('‚ùå Erro ao resolver tool calls iniciais:', toolError);
              }
            }

            let textChunkCount = 0;
            let toolCallCount = 0;
            let hasProcessedToolCalls = false;

            // Processar stream completo incluindo texto ap√≥s tool calls
            console.log('üåä Iniciando itera√ß√£o do fullStream...');
            let chunkIndex = 0;
            for await (const chunk of streamResult.fullStream) {
              chunkIndex++;
              console.log(`üîÑ Processando chunk ${chunkIndex} do fullStream:`, {
                type: chunk.type,
                hasText: chunk.type === 'text-delta' ? !!chunk.text : false,
                hasToolCall: chunk.type === 'tool-call' ? !!chunk.toolName : false
              });
              
              if (chunk.type === 'text-delta') {
                textChunkCount++;
                console.log(`üìù Chunk de texto ${textChunkCount}:`, chunk.text);
                const data = JSON.stringify({
                  type: 'text',
                  content: chunk.text,
                  sessionId: finalSessionId,
                  timestamp: new Date().toISOString(),
                });
                controller.enqueue(new TextEncoder().encode(`data: ${data}\n\n`));
              } else if (chunk.type === 'tool-call') {
                toolCallCount++;
                hasProcessedToolCalls = true;
                console.log(`üõ†Ô∏è Processando tool call ${toolCallCount}:`, {
                    toolName: chunk.toolName,
                    toolCallId: chunk.toolCallId
                  });
                
                try {
                  const toolData = JSON.stringify({
                    type: 'tool_call',
                    toolCall: chunk,
                    sessionId: finalSessionId,
                    timestamp: new Date().toISOString(),
                  });
                  console.log(`üì§ Enviando tool call data:`, toolData.substring(0, 200) + '...');
                  controller.enqueue(new TextEncoder().encode(`data: ${toolData}\n\n`));
                  console.log(`‚úÖ Tool call ${toolCallCount} enviado com sucesso`);
                } catch (toolError) {
                  console.error(`‚ùå Erro ao processar tool call ${toolCallCount}:`, toolError);
                }
              } else if (chunk.type === 'tool-result') {
                console.log(`üîß Tool result recebido para ${chunk.toolCallId}`);
              }
            }

            console.log(`üìä Total de chunks de texto processados: ${textChunkCount}`);
            console.log(`üìä Total de tool calls processados: ${toolCallCount}`);
            console.log(`üîß Tool calls foram processados: ${hasProcessedToolCalls}`);
            console.log(`üìä Total de chunks processados: ${chunkIndex}`);
            
            // Detectar stream vazio
            if (chunkIndex === 0) {
              console.log('‚ö†Ô∏è PROBLEMA DETECTADO: Stream est√° completamente vazio!');
              console.log('‚ö†Ô∏è Isso indica um problema com o modelo LLM ou configura√ß√£o');
            }

            // Solu√ß√£o h√≠brida: Se tool calls foram processados mas nenhum texto foi gerado,
            // for√ßa uma segunda chamada ao modelo para gerar resposta textual
            if (hasProcessedToolCalls && textChunkCount === 0) {
              console.log('üîÑ Implementando solu√ß√£o h√≠brida: for√ßando resposta textual ap√≥s tool calls');
              try {
                const followUpResult = await agent.processMessage(
                  finalSessionId,
                  'Por favor, explique o que foi feito com base nos resultados das ferramentas executadas.',
                  context
                );
                
                if (followUpResult && followUpResult.trim()) {
                  console.log('üìù Resposta textual for√ßada gerada:', followUpResult.substring(0, 100) + '...');
                  const followUpData = JSON.stringify({
                    type: 'text',
                    content: followUpResult,
                    sessionId: finalSessionId,
                    timestamp: new Date().toISOString(),
                  });
                  controller.enqueue(new TextEncoder().encode(`data: ${followUpData}\n\n`));
                } else {
                  console.log('‚ö†Ô∏è Resposta textual for√ßada est√° vazia');
                }
              } catch (followUpError) {
                console.error('‚ùå Erro ao gerar resposta textual for√ßada:', followUpError);
                // Enviar uma resposta padr√£o em caso de erro
                const fallbackData = JSON.stringify({
                  type: 'text',
                  content: 'A√ß√£o executada com sucesso! Como posso ajud√°-lo mais?',
                  sessionId: finalSessionId,
                  timestamp: new Date().toISOString(),
                });
                controller.enqueue(new TextEncoder().encode(`data: ${fallbackData}\n\n`));
              }
            }

            // Finalizar stream
            console.log('üèÅ Finalizando stream');
            const endData = JSON.stringify({
              type: 'end',
              sessionId: finalSessionId,
              timestamp: new Date().toISOString(),
            });
            controller.enqueue(new TextEncoder().encode(`data: ${endData}\n\n`));
            controller.close();
            console.log('‚úÖ Stream finalizado com sucesso');
          } catch (error) {
            console.error('‚ùå Erro durante streaming:', error);
            console.error('‚ùå Stack trace:', error instanceof Error ? error.stack : 'Stack n√£o dispon√≠vel');
            const errorData = JSON.stringify({
              type: 'error',
              error: 'Erro interno do servidor',
              content: 'Erro interno do servidor',
              sessionId: finalSessionId 
            });
            controller.enqueue(new TextEncoder().encode(`data: ${errorData}\n\n`));
            controller.close();
          }
        },
      });

      return new Response(stream, {
        headers: {
          'Content-Type': 'text/event-stream',
          'Cache-Control': 'no-cache',
          'Connection': 'keep-alive',
          'Access-Control-Allow-Origin': '*',
          'Access-Control-Allow-Methods': 'POST, OPTIONS',
          'Access-Control-Allow-Headers': 'Content-Type',
        },
      });
    } else {
      // Resposta normal (n√£o streaming)
      const response = await agent.processMessage(finalSessionId, message, context);
      
      return NextResponse.json({
        response,
        sessionId: finalSessionId,
        timestamp: new Date().toISOString(),
      });
    }
  } catch (error) {
    console.error('Erro na API ai-chat:', error);
    return NextResponse.json(
      { 
        error: 'Erro interno do servidor',
        details: process.env.NODE_ENV === 'development' ? String(error) : undefined
      },
      { status: 500 }
    );
  }
}

// GET - Obter hist√≥rico da sess√£o
export async function GET(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const sessionId = searchParams.get('sessionId');
    const action = searchParams.get('action');

    if (!sessionId) {
      return NextResponse.json(
        { error: 'SessionId √© obrigat√≥rio' },
        { status: 400 }
      );
    }

    const agent = getPharmacyAgent();

    switch (action) {
      case 'history':
        const history = agent.getSessionHistory(sessionId);
        return NextResponse.json({ history, sessionId });
      
      case 'context':
        const context = agent.getSessionContext(sessionId);
        return NextResponse.json({ context, sessionId });
      
      case 'config':
        const config = agent.getLLMConfig();
        return NextResponse.json({ config });
      
      default:
        return NextResponse.json(
          { error: 'A√ß√£o n√£o suportada. Use: history, context, ou config' },
          { status: 400 }
        );
    }
  } catch (error) {
    console.error('Erro na API ai-chat GET:', error);
    return NextResponse.json(
      { 
        error: 'Erro interno do servidor',
        details: process.env.NODE_ENV === 'development' ? String(error) : undefined
      },
      { status: 500 }
    );
  }
}

// DELETE - Limpar sess√£o
export async function DELETE(request: NextRequest) {
  try {
    const { searchParams } = new URL(request.url);
    const sessionId = searchParams.get('sessionId');

    if (!sessionId) {
      return NextResponse.json(
        { error: 'SessionId √© obrigat√≥rio' },
        { status: 400 }
      );
    }

    const agent = getPharmacyAgent();
    agent.clearSession(sessionId);

    return NextResponse.json({ 
      message: 'Sess√£o limpa com sucesso',
      sessionId 
    });
  } catch (error) {
    console.error('Erro na API ai-chat DELETE:', error);
    return NextResponse.json(
      { 
        error: 'Erro interno do servidor',
        details: process.env.NODE_ENV === 'development' ? String(error) : undefined
      },
      { status: 500 }
    );
  }
}

// PUT - Atualizar configura√ß√£o do LLM
export async function PUT(request: NextRequest) {
  try {
    const body = await request.json();
    const { llmConfig } = body;

    if (!llmConfig) {
      return NextResponse.json(
        { error: 'Configura√ß√£o do LLM √© obrigat√≥ria' },
        { status: 400 }
      );
    }

    const agent = getPharmacyAgent();
    agent.updateLLMConfig(llmConfig);

    return NextResponse.json({ 
      message: 'Configura√ß√£o atualizada com sucesso',
      config: agent.getLLMConfig()
    });
  } catch (error) {
    console.error('Erro na API ai-chat PUT:', error);
    return NextResponse.json(
      { 
        error: 'Erro interno do servidor',
        details: process.env.NODE_ENV === 'development' ? String(error) : undefined
      },
      { status: 500 }
    );
  }
}

// OPTIONS - CORS
export async function OPTIONS() {
  return new Response(null, {
    status: 200,
    headers: {
      'Access-Control-Allow-Origin': '*',
      'Access-Control-Allow-Methods': 'GET, POST, PUT, DELETE, OPTIONS',
      'Access-Control-Allow-Headers': 'Content-Type, Authorization',
    },
  });
}