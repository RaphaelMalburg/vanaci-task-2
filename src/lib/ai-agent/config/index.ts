import { openai } from "@ai-sdk/openai";
import { google } from "@ai-sdk/google";
import { anthropic } from "@ai-sdk/anthropic";
import { mistral } from "@ai-sdk/mistral";

// Tipos para configura√ß√£o
export interface LLMConfig {
  provider: "openai" | "google" | "anthropic" | "mistral";
  model?: string;
  temperature?: number;
  maxTokens?: number;
  enableMessageRewriter?: boolean;
}

// Configura√ß√µes padr√£o dos modelos
const DEFAULT_MODELS = {
  openai: process.env.OPENAI_MODEL || "gpt-4-turbo-preview",
  google: process.env.GEMINI_MODEL || "gemini-1.5-flash",
  anthropic: process.env.ANTHROPIC_MODEL || "claude-3-sonnet-20240229",
  mistral: process.env.MISTRAL_MODEL || "mistral-large-latest",
};

// Configura√ß√µes padr√£o
const DEFAULT_CONFIG: LLMConfig = {
  provider: (process.env.DEFAULT_LLM_PROVIDER as LLMConfig["provider"]) || "openai",
  temperature: parseFloat(process.env.LLM_TEMPERATURE || "0.7"),
  maxTokens: parseInt(process.env.LLM_MAX_TOKENS || "2000"),
  enableMessageRewriter: process.env.ENABLE_MESSAGE_REWRITER === "true",
};

/**
 * Cria uma inst√¢ncia do modelo LLM baseado na configura√ß√£o
 */
export async function createLLMModel(config: Partial<LLMConfig> = {}) {
  console.log('üîß createLLMModel: Iniciando cria√ß√£o do modelo');
  console.log('üìã Configura√ß√£o recebida:', config);
  
  const finalConfig = { ...DEFAULT_CONFIG, ...config };
  const modelName = finalConfig.model || DEFAULT_MODELS[finalConfig.provider];
  
  console.log('‚öôÔ∏è Configura√ß√£o final:', finalConfig);
  console.log('üè∑Ô∏è Nome do modelo:', modelName);
  console.log('üîë Provider selecionado:', finalConfig.provider);

  switch (finalConfig.provider) {
    case "openai":
      console.log('üîç Verificando OPENAI_API_KEY...');
      if (!process.env.OPENAI_API_KEY) {
        console.log('‚ùå OPENAI_API_KEY n√£o encontrada');
        throw new Error("OPENAI_API_KEY n√£o configurada");
      }
      console.log('‚úÖ Criando modelo OpenAI:', modelName);
      return openai(modelName);

    case "google":
      console.log('üîç Verificando GOOGLE_GENERATIVE_AI_API_KEY...');
      if (!process.env.GOOGLE_GENERATIVE_AI_API_KEY) {
        console.log('‚ùå GOOGLE_GENERATIVE_AI_API_KEY n√£o encontrada');
        throw new Error("GOOGLE_GENERATIVE_AI_API_KEY n√£o configurada");
      }
      console.log('‚úÖ Criando modelo Google:', modelName);
      return google(modelName);

    case "anthropic":
      console.log('üîç Verificando ANTHROPIC_API_KEY...');
      if (!process.env.ANTHROPIC_API_KEY) {
        console.log('‚ùå ANTHROPIC_API_KEY n√£o encontrada');
        throw new Error("ANTHROPIC_API_KEY n√£o configurada");
      }
      console.log('‚úÖ Criando modelo Anthropic:', modelName);
      return anthropic(modelName);

    case "mistral":
      console.log('üîç Verificando MISTRAL_API_KEY...');
      if (!process.env.MISTRAL_API_KEY) {
        console.log('‚ùå MISTRAL_API_KEY n√£o encontrada');
        throw new Error("MISTRAL_API_KEY n√£o configurada");
      }
      console.log('‚úÖ Criando modelo Mistral:', modelName);
      const model = mistral(modelName);
      console.log('üéØ Modelo Mistral criado com sucesso');
      return model;

    default:
      console.log('‚ùå Provider n√£o suportado:', finalConfig.provider);
      throw new Error(`Provedor LLM n√£o suportado: ${finalConfig.provider}`);
  }
}

/**
 * Cria modelo com fallback autom√°tico para outros provedores
 */
export async function createLLMModelWithFallback(config: Partial<LLMConfig> = {}) {
  console.log('üîÑ createLLMModelWithFallback: Iniciando cria√ß√£o com fallback');
  
  const availableProviders = getAvailableProviders();
  const primaryProvider = config.provider || DEFAULT_CONFIG.provider;
  
  // Tenta o provider prim√°rio primeiro
  if (availableProviders.includes(primaryProvider)) {
    try {
      console.log(`üéØ Tentando provider prim√°rio: ${primaryProvider}`);
      return await createLLMModel({ ...config, provider: primaryProvider });
    } catch (error) {
      console.log(`‚ö†Ô∏è Falha no provider ${primaryProvider}:`, error);
    }
  }
  
  // Tenta outros providers dispon√≠veis como fallback
  for (const provider of availableProviders) {
    if (provider !== primaryProvider) {
      try {
        console.log(`üîÑ Tentando fallback para: ${provider}`);
        return await createLLMModel({ ...config, provider });
      } catch (error) {
        console.log(`‚ö†Ô∏è Falha no fallback ${provider}:`, error);
      }
    }
  }
  
  throw new Error('‚ùå Nenhum provider LLM dispon√≠vel. Verifique as chaves de API.');
}

/**
 * Verifica se as chaves de API necess√°rias est√£o configuradas
 */
export function validateLLMConfig(provider?: LLMConfig["provider"]) {
  const targetProvider = provider || DEFAULT_CONFIG.provider;

  const requiredKeys = {
    openai: "OPENAI_API_KEY",
    google: "GOOGLE_GENERATIVE_AI_API_KEY",
    anthropic: "ANTHROPIC_API_KEY",
    mistral: "MISTRAL_API_KEY",
  };

  const requiredKey = requiredKeys[targetProvider];
  if (!process.env[requiredKey]) {
    throw new Error(`Chave de API n√£o configurada: ${requiredKey}`);
  }

  return true;
}

/**
 * Lista os provedores dispon√≠veis baseado nas chaves configuradas
 */
export function getAvailableProviders(): LLMConfig["provider"][] {
  const providers: LLMConfig["provider"][] = [];

  if (process.env.OPENAI_API_KEY) providers.push("openai");
  if (process.env.GOOGLE_GENERATIVE_AI_API_KEY) providers.push("google");
  if (process.env.ANTHROPIC_API_KEY) providers.push("anthropic");
  if (process.env.MISTRAL_API_KEY) providers.push("mistral");

  return providers;
}

export { DEFAULT_CONFIG, DEFAULT_MODELS };
