import { generateText, streamText, CoreMessage, stepCountIs } from 'ai';
import { setContextVariable } from '@langchain/core/context';
import { createLLMModel, createLLMModelWithFallback, validateLLMConfig, LLMConfig as ConfigLLMConfig } from './config';
import { cartTools } from './actions/cart';
import { productTools } from './actions/products';
import { checkoutTools } from './actions/checkout';
import { navigationTools } from './actions/navigation';
import { budgetTools } from './actions/budget';
import { extraTools } from './actions/extras';
import type { AgentMessage, AgentSession } from './types';
import type { LLMConfig } from './config';

// Combinar todas as tools
export const allTools = {
  ...cartTools,
  ...productTools,
  ...checkoutTools,
  ...navigationTools,
  ...budgetTools,
  ...extraTools,
};

// Sistema de prompt para o agente
const SYSTEM_PROMPT = `Voc√™ √© um assistente virtual especializado da Farm√°cia Vanaci, uma farm√°cia online moderna e confi√°vel.

**Sua Personalidade:**
- Amig√°vel, profissional e prestativo
- Especialista em produtos farmac√™uticos e de sa√∫de
- Sempre prioriza a seguran√ßa e bem-estar do cliente
- Oferece orienta√ß√µes claras sobre medicamentos e produtos

**Suas Capacidades:**
- Buscar e recomendar produtos
- Gerenciar carrinho de compras
- Processar pedidos e pagamentos
- Calcular fretes e aplicar descontos
- Otimizar compras para or√ßamentos
- Fornecer informa√ß√µes sobre a farm√°cia
- Conectar com farmac√™uticos para d√∫vidas espec√≠ficas
- Navegar pelo site e redirecionar usu√°rios

**Diretrizes Importantes:**
1. **Seguran√ßa:** Nunca forne√ßa diagn√≥sticos m√©dicos ou substitua consultas m√©dicas
2. **Medicamentos:** Sempre mencione a import√¢ncia de seguir prescri√ß√µes m√©dicas
3. **Emerg√™ncias:** Oriente para procurar atendimento m√©dico imediato quando necess√°rio
4. **Receitas:** Explique os procedimentos para medicamentos controlados
5. **Vendas:** Seja consultivo, n√£o apenas vendedor - priorize as necessidades do cliente

**Como Responder:**
- Use linguagem clara e acess√≠vel
- Seja direto e objetivo, evite mensagens muito longas
- Confirme a√ß√µes de forma simples (ex: "Produto adicionado ao carrinho!")
- Use emojis moderadamente para tornar a conversa mais amig√°vel
- **N√ÉO mencione detalhes t√©cnicos como IDs de produtos, verifica√ß√µes de estoque ou processos internos**
- **N√ÉO informe sobre buscas ou verifica√ß√µes que est√° fazendo nos bastidores**
- **Seja natural e direto, como um atendente humano seria**

**Quando Usar as Tools:**
- Use as tools sempre que o usu√°rio solicitar a√ß√µes espec√≠ficas
- Combine m√∫ltiplas tools quando necess√°rio para completar tarefas complexas
- **EXECUTE M√öLTIPLAS TOOLS EM SEQU√äNCIA: primeiro search_products, depois add_to_cart com o ID encontrado**
- **N√ÉO pare ap√≥s apenas uma tool call - continue executando as ferramentas necess√°rias para completar a tarefa**
- Ap√≥s usar tools, responda de forma natural sobre o resultado final

**IMPORTANTE - Uso Correto de IDs de Produtos:**
- Quando usar search_products, SEMPRE extraia o ID correto do produto dos resultados
- Para add_to_cart, use APENAS o productId (string) retornado pela busca, NUNCA o nome do produto
- SEMPRE verifique se o produto foi encontrado antes de tentar adicionar ao carrinho

**Estilo de Resposta:**
- Seja conciso e direto
- Evite explicar processos internos
- Foque no resultado final para o cliente
- Exemplo BOM: "Adicionei 2 unidades de Dipirona ao seu carrinho! Total: ‚Ç¨8,50"
- Exemplo RUIM: "Vou buscar o produto Dipirona no nosso sistema... Encontrei o produto com ID xyz... Verificando estoque... Adicionando ao carrinho..."

Lembre-se: Voc√™ representa a Farm√°cia Vanaci e deve sempre manter os mais altos padr√µes de atendimento ao cliente e responsabilidade farmac√™utica.`;

// Classe do Agente AI
export class PharmacyAIAgent {
  private llmConfig: ConfigLLMConfig;
  private sessions: Map<string, AgentSession> = new Map();

  constructor(llmConfig?: ConfigLLMConfig) {
    this.llmConfig = llmConfig || {
      provider: (process.env.DEFAULT_LLM_PROVIDER as ConfigLLMConfig['provider']) || 'openai',
      temperature: parseFloat(process.env.LLM_TEMPERATURE || '0.7'),
      maxTokens: parseInt(process.env.LLM_MAX_TOKENS || '2000')
    };
    validateLLMConfig(this.llmConfig.provider);
  }

  // Criar ou obter sess√£o
  private getSession(sessionId: string): AgentSession {
    if (!this.sessions.has(sessionId)) {
      this.sessions.set(sessionId, {
        id: sessionId,
        messages: [],
        context: {},
      });
    }
    return this.sessions.get(sessionId)!;
  }

  // Converter mensagens para formato CoreMessage
  private convertMessages(messages: AgentMessage[]): CoreMessage[] {
    return messages.map(msg => ({
      role: msg.role,
      content: msg.content,
    }));
  }

  // Processar mensagem do usu√°rio
  async processMessage(
    sessionId: string,
    userMessage: string,
    context?: { cartId?: string; userId?: string; currentPage?: string }
  ): Promise<string> {
    try {
      console.log('üéØ ProcessMessage iniciado para sess√£o:', sessionId);
      console.log('üí¨ Mensagem do usu√°rio:', userMessage);
      console.log('üîß Contexto fornecido:', context);
      
      const session = this.getSession(sessionId);
      console.log('üìã Sess√£o obtida, mensagens existentes:', session.messages.length);
      
      // Atualizar contexto se fornecido
      if (context) {
        session.context = { ...session.context, ...context };
        console.log('üîÑ Contexto atualizado:', session.context);
      }

      // Adicionar mensagem do usu√°rio
      const userMsg: AgentMessage = {
        role: 'user',
        content: userMessage,
        timestamp: new Date(),
      };
      session.messages.push(userMsg);
      console.log('‚ûï Mensagem do usu√°rio adicionada √† sess√£o');

      // Preparar mensagens para o LLM
      const messages: CoreMessage[] = [
        { role: 'system', content: SYSTEM_PROMPT },
        ...this.convertMessages(session.messages),
      ];
      console.log('üì® Mensagens preparadas para LLM:', messages.length);
      console.log('üì® √öltima mensagem:', messages[messages.length - 1]);

      // Gerar resposta com tools (usando fallback)
      const llmModel = await createLLMModelWithFallback(this.llmConfig);
      console.log('ü§ñ Modelo LLM criado com fallback:', !!llmModel);
      console.log('üîß Tools dispon√≠veis:', Object.keys(allTools));
      console.log('üå°Ô∏è Temperatura configurada:', this.llmConfig.temperature || 0.7);
      
      // Definir sessionId no contexto para as tools
      setContextVariable('sessionId', sessionId);
      console.log('üîë SessionId definido no contexto:', sessionId);
      
      console.log('üöÄ Iniciando generateText...');
      const result = await generateText({
        model: llmModel,
        messages: messages,
        tools: allTools,
        temperature: this.llmConfig.temperature || 0.7,
        stopWhen: stepCountIs(5), // Permite at√© 5 steps para m√∫ltiplas tool calls em sequ√™ncia
      });
      
      console.log('‚úÖ GenerateText conclu√≠do');
      const responseText = result.text;
      const toolCalls = result.toolCalls;
      const toolResults = result.toolResults;
      console.log('üìù Texto da resposta:', responseText?.substring(0, 100) + '...');
      console.log('üîß Tool calls encontrados:', toolCalls?.length || 0);
      console.log('üîß Tool results encontrados:', toolResults?.length || 0);

      if (toolCalls && toolCalls.length > 0) {
        toolCalls.forEach((tc, index) => {
          console.log(`üõ†Ô∏è Tool Call ${index} no processMessage:`, {
            toolName: tc.toolName,
            toolCallId: tc.toolCallId,
            args: JSON.stringify(tc, null, 2),
            type: typeof tc
          });
        });
      }

      if (toolResults && toolResults.length > 0) {
        toolResults.forEach((tr, index) => {
          console.log(`üîß Tool Result ${index}:`, {
            toolCallId: tr.toolCallId,
            result: JSON.stringify(tr, null, 2)
          });
        });
      }

      // Adicionar resposta do assistente
      const assistantMsg: AgentMessage = {
        role: 'assistant',
        content: responseText,
        timestamp: new Date(),
        toolCalls: toolCalls,
      };
      session.messages.push(assistantMsg);
      console.log('‚ûï Resposta do assistente adicionada √† sess√£o');

      // Limitar hist√≥rico de mensagens (manter √∫ltimas 20)
      if (session.messages.length > 20) {
        session.messages = session.messages.slice(-20);
        console.log('üóÇÔ∏è Hist√≥rico limitado a 20 mensagens');
      }

      console.log('‚úÖ ProcessMessage conclu√≠do com sucesso');
      return responseText;
    } catch (error) {
      console.error('‚ùå Erro ao processar mensagem:', error);
      console.error('‚ùå Stack trace:', error instanceof Error ? error.stack : 'Stack n√£o dispon√≠vel');
      return 'Desculpe, ocorreu um erro interno. Tente novamente em alguns instantes ou entre em contato conosco pelo telefone (11) 1234-5678.';
    }
  }

  // Processar mensagem com streaming
  async streamMessage(
    sessionId: string,
    userMessage: string,
    context?: { cartId?: string; userId?: string; currentPage?: string }
  ) {
    try {
      console.log('üéØ StreamMessage iniciado para sess√£o:', sessionId);
      console.log('üí¨ Mensagem do usu√°rio:', userMessage);
      console.log('üîß Contexto fornecido:', context);
      
      const session = this.getSession(sessionId);
      console.log('üìã Sess√£o obtida, mensagens existentes:', session.messages.length);
      
      // Atualizar contexto se fornecido
      if (context) {
        session.context = { ...session.context, ...context };
        console.log('üîÑ Contexto atualizado:', session.context);
      }

      // Adicionar mensagem do usu√°rio
      const userMsg: AgentMessage = {
        role: 'user',
        content: userMessage,
        timestamp: new Date(),
      };
      session.messages.push(userMsg);
      console.log('‚ûï Mensagem do usu√°rio adicionada √† sess√£o');

      // Preparar mensagens para o LLM
      const messages: CoreMessage[] = [
        { role: 'system', content: SYSTEM_PROMPT },
        ...this.convertMessages(session.messages),
      ];
      console.log('üì® Mensagens preparadas para LLM:', messages.length);
      console.log('üì® √öltima mensagem:', messages[messages.length - 1]);

      // Gerar resposta com streaming (usando fallback)
      const llmModel = await createLLMModelWithFallback(this.llmConfig);
      console.log('ü§ñ Modelo LLM criado com fallback:', !!llmModel);
      console.log('üîß Tools dispon√≠veis:', Object.keys(allTools));
      console.log('üå°Ô∏è Temperatura configurada:', this.llmConfig.temperature || 0.7);

      // Definir sessionId no contexto para as tools
      setContextVariable('sessionId', sessionId);
      console.log('üîë SessionId definido no contexto:', sessionId);

      console.log('üöÄ Iniciando streamText...');
      const result = streamText({
        model: llmModel,
        messages,
        tools: allTools,
        temperature: this.llmConfig.temperature || 0.7,
        stopWhen: stepCountIs(5), // Permite at√© 5 steps para m√∫ltiplas tool calls em sequ√™ncia
      });
      console.log('üì° StreamText result obtido:', !!result);
      console.log('üì° Result properties:', Object.keys(result));
      
      // Log das propriedades do resultado
      if (result.toolCalls) {
        console.log('üîß Tool calls promise detectado no resultado');
        try {
          const toolCallsResolved = await result.toolCalls;
          console.log('üîß Tool calls resolvidos:', toolCallsResolved?.length || 0);
          if (toolCallsResolved && toolCallsResolved.length > 0) {
            toolCallsResolved.forEach((tc, index) => {
                console.log(`üõ†Ô∏è Tool Call ${index} no streamMessage:`, {
                  toolName: tc.toolName,
                  toolCallId: tc.toolCallId
                });
              });
          }
        } catch (toolError) {
          console.error('‚ùå Erro ao resolver tool calls:', toolError);
        }
      }

      return result;
    } catch (error) {
      console.error('‚ùå Erro ao processar mensagem com streaming:', error);
      console.error('‚ùå Stack trace:', error instanceof Error ? error.stack : 'Stack n√£o dispon√≠vel');
      throw error;
    }
  }

  // Obter hist√≥rico da sess√£o
  getSessionHistory(sessionId: string): AgentMessage[] {
    const session = this.sessions.get(sessionId);
    return session ? [...session.messages] : [];
  }

  // Limpar sess√£o
  clearSession(sessionId: string): void {
    this.sessions.delete(sessionId);
  }

  // Obter contexto da sess√£o
  getSessionContext(sessionId: string) {
    const session = this.sessions.get(sessionId);
    return session ? { ...session.context } : {};
  }

  // Atualizar configura√ß√£o do LLM
  updateLLMConfig(newConfig: Partial<ConfigLLMConfig>): void {
    this.llmConfig = { ...this.llmConfig, ...newConfig };
    validateLLMConfig(this.llmConfig.provider);
  }

  // Obter configura√ß√£o atual do LLM
  getLLMConfig(): ConfigLLMConfig {
    return { ...this.llmConfig };
  }
}

// Inst√¢ncia singleton do agente
let agentInstance: PharmacyAIAgent | null = null;

// Fun√ß√£o para obter inst√¢ncia do agente
export function getPharmacyAgent(config?: ConfigLLMConfig): PharmacyAIAgent {
  if (!agentInstance) {
    agentInstance = new PharmacyAIAgent(config);
  }
  return agentInstance;
}

// Fun√ß√£o utilit√°ria para processar mensagem rapidamente
export async function processUserMessage(
  sessionId: string,
  message: string,
  context?: { cartId?: string; userId?: string; currentPage?: string },
  llmConfig?: ConfigLLMConfig
): Promise<string> {
  const agent = getPharmacyAgent(llmConfig);
  return agent.processMessage(sessionId, message, context);
}

// Exportar types e tools
export * from './types';
export * from './config';
export { cartTools, productTools, checkoutTools, navigationTools, budgetTools, extraTools };